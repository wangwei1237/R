ID,title,url,date,topic,keyworkds
1,GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework,https://arxiv.org/abs/2407.10793,Jul-24,通用,
2,KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions,https://arxiv.org/abs/2407.05868﻿,Jul-24,通用,
3,DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation,https://arxiv.org/abs/2406.09155,Jun-24,通用,
4,A Survey of Useful LLM Evaluation,https://arxiv.org/abs/2406.00936,Jun-24,通用,
5,Is Your LLM Outdated? Evaluating LLMs at Temporal Generalization,https://arxiv.org/abs/2405.08460,May-24,通用,
6,Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions,https://arxiv.org/abs/2404.09135,Apr-24,通用,
7,Evaluating LLMs at Detecting Errors in LLM Responses,https://arxiv.org/abs/2404.03602,Apr-24,通用,
8,Exploring and Evaluating Hallucinations in LLM-Powered Code Generation,https://arxiv.org/abs/2404.00971,Apr-24,代码,
9,On Evaluating the Efficiency of Source Code Generated by LLMs,https://arxiv.org/abs/2404.06041,Apr-24,代码,
10,Evaluating the Performance of LLMs on Technical Language Processing tasks,https://arxiv.org/abs/2403.15503,Mar-24,通用,
11,Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach,https://arxiv.org/abs/2403.15250,Mar-24,通用,
12,Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference,https://arxiv.org/abs/2403.04132,Mar-24,通用,
13,FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability,https://arxiv.org/abs/2402.18667,Mar-24,通用,
14,"Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot",https://arxiv.org/abs/2403.11381,Mar-24,通用,
15,Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs,https://arxiv.org/abs/2402.17649,Feb-24,通用,
16,tinyBenchmarks: evaluating LLMs with fewer examples,https://arxiv.org/abs/2402.14992,Feb-24,通用,
17,GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data,https://arxiv.org/abs/2402.14973,Feb-24,通用,
18,Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming,https://arxiv.org/abs/2402.14261,Feb-24,通用,
19,TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization,https://arxiv.org/abs/2402.13249,Feb-24,通用,
20,TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness,https://arxiv.org/abs/2402.12545,Feb-24,通用,
21,Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation,https://arxiv.org/abs/2402.11443,Feb-24,通用,
22,Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering,https://arxiv.org/abs/2402.11194,Feb-24,通用,
23,LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models,https://arxiv.org/abs/2402.10524,Feb-24,通用,
24,AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability,https://arxiv.org/abs/2402.09404,Feb-24,通用,
25,Conversational Assistants in Knowledge-Intensive Contexts: An Evaluation of LLM- versus Intent-based Systems,https://arxiv.org/abs/2402.04955,Feb-24,通用,
26,"""Which LLM should I use?"": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students",https://arxiv.org/abs/2402.01687,Feb-24,通用,
27,Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions,https://arxiv.org/abs/2401.09395,Jan-24,通用,
28,Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs,https://arxiv.org/abs/2401.05940,Jan-24,代码,
29,PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs,https://arxiv.org/abs/2401.03855,Jan-24,代码,
30,How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation,https://arxiv.org/abs/2312.17115,Dec-23,通用,
31,DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents,https://arxiv.org/abs/2311.09805,Nov-23,通用,
32,CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,https://arxiv.org/abs/2311.08588,Nov-23,代码,
33,KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval,https://arxiv.org/abs/2310.15511,Oct-23,通用,
34,BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues,https://arxiv.org/abs/2310.13650,Oct-23,通用,
35,A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing,https://arxiv.org/abs/2310.08433,Oct-23,通用,
36,LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models,https://arxiv.org/abs/2310.03903,Dec-23,通用,
37,Safurai 001: New Qualitative Approach for Code LLM Evaluation,https://arxiv.org/abs/2309.11385,Sep-23,代码,
38,MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback,https://arxiv.org/abs/2309.10691,Sep-23,通用,
39,LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins,https://arxiv.org/abs/2309.10254,Sep-23,通用,
40,Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment,https://arxiv.org/abs/2308.05374,Aug-23,通用,
41,AgentBench: Evaluating LLMs as Agents,https://arxiv.org/abs/2308.03688,Aug-23,通用,
42,ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation,https://arxiv.org/abs/2308.01861,Aug-23,代码,
43,Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs,https://arxiv.org/abs/2306.13063,Jun-23,通用,
44,Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET,https://arxiv.org/abs/2305.14938,May-23,通用,
45,Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,https://arxiv.org/abs/2305.13160,May-23,通用,
46,"GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts",https://arxiv.org/abs/2305.12477,May-23,通用,
47,Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction,https://arxiv.org/abs/2305.06474,May-23,通用,
48,A Survey on the Memory Mechanism of Large Language Model based Agents,https://arxiv.org/html/2404.13501v1,Apr-24,记忆,
49,Aspects of human memory and Large Language Models,https://arxiv.org/pdf/2311.03839,Apr-24,记忆,
50,Evaluating Very Long-Term Conversational Memory of LLM Agents,https://arxiv.org/abs/2402.17753,Feb-24,记忆,
51,Large Language Models (LLM) with Long-Term Memory: Advancements and Opportunities in GenAI Applications,https://yulleyi.medium.com/large-language-models-llm-with-long-term-memory-advancements-and-opportunities-in-genai-fcc3590f1c0e,Jul-23,记忆,
52,"ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",https://arxiv.org/abs/2408.04682,Aug-24,工具,
53,PersonaGym: Evaluating Persona Agents and LLMs,https://arxiv.org/abs/2407.18416,Jul-24,角色扮演,
54,Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using LLMs with Human in the Loop,https://arxiv.org/abs/2407.05925,Jul-24,RAG,
55,Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need,https://arxiv.org/abs/2406.18064,Jun-24,RAG,
56,Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability,https://arxiv.org/abs/2406.11424,Jun-24,RAG,
57,Evaluating the Retrieval Component in LLM-Based Question Answering Systems,https://arxiv.org/abs/2406.06458,Jun-24,RAG,
58,How Well Can LLMs Echo Us? Evaluating AI Chatbots' Role-Play Ability with ECHO,https://arxiv.org/abs/2404.13957,Apr-24,角色扮演,
59,ragas,https://github.com/explodinggradients/ragas,May-23,RAG,
60,A Survey of Intent Classification and Slot-Filling Datasets for Task-Oriented Dialog,https://arxiv.org/pdf/2207.13211,Jul-22,意图识别,
61,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,https://www.isca-archive.org/interspeech_2023/he23_interspeech.pdf,May-23,意图识别,
62,Understanding user intent modeling for conversational recommender systems: a systematic literature review,https://link.springer.com/article/10.1007/s11257-024-09398-x,Jun-24,意图识别,
63,"Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs",https://arxiv.org/abs/2408.01417,Aug-24,意图识别,
64,K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences,https://arxiv.org/abs/2408.14468,Aug-24,多模态,
65,Foundation Models for Music: A Survey,https://arxiv.org/abs/2408.14340,Aug-24,多模态,
66,I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing,https://arxiv.org/abs/2408.14180,Aug-24,多模态,
67,Revisiting Multi-Modal LLM Evaluation,https://arxiv.org/abs/2408.05334,Aug-24,多模态,
68,User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance,https://arxiv.org/abs/2408.03160,Aug-24,多模态,
69,"Can LLMs ""Reason"" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation",https://arxiv.org/abs/2407.21531,Aug-24,多模态,
70,DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems,https://arxiv.org/abs/2407.10701,Jul-24,多模态,
71,EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models,https://arxiv.org/abs/2406.16562,Jun-24,多模态,
72,GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs GSR-BENCH,https://arxiv.org/abs/2406.13246,Jun-24,多模态,
73,Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis,https://arxiv.org/abs/2405.21075,Jun-24,多模态,
74,Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs,https://arxiv.org/abs/2402.12424,Feb-24,多模态,
75,MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria,https://arxiv.org/abs/2311.13951,Nov-23,多模态,
76,AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation,https://arxiv.org/abs/2311.07397,Nov-23,多模态,
77, SWE-Bench Verified,https://openai.com/index/introducing-swe-bench-verified/,Aug-24,数据集构建,
78,SWE-Bench：SWE-bench: Can Language Models Resolve Real-World GitHub Issues,https://arxiv.org/abs/2310.06770,Apr-24,数据集构建,
79,SWE-bench-java: A GitHub Issue Resolving Benchmark for Java,https://arxiv.org/abs/2408.14354﻿,Aug-24,数据集构建,
80,Assessing Contamination in Large Language Models: Introducing the LogProber method,https://arxiv.org/abs/2408.14352,Aug-24,数据集构建,
81,Using Large Language Models to Document Code: A First Quantitative and Qualitative Assessment,https://arxiv.org/abs/2408.14007,Aug-24,数据集构建,
82,DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation,https://arxiv.org/abs/2408.13204,Aug-24,数据集构建,
83,CIBench: Evaluating Your LLMs with a Code Interpreter Plugin,https://arxiv.org/abs/2407.10499﻿,Jul-24,数据集构建,
84,PyBench: Evaluating LLM Agent on various real-world coding tasks,https://arxiv.org/abs/2407.16732,Jul-24,数据集构建,
85,Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark Evaluation,https://arxiv.org/abs/2407.13696,Jul-24,数据集构建,
86,MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs,https://arxiv.org/abs/2407.01509,Jul-24,数据集构建,
87,Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning,https://arxiv.org/abs/2406.09170,Jun-24,数据集构建,
88,MARIO Eval: Evaluate Your Math LLM with your Math LLM--A mathematical dataset evaluation toolkit,https://arxiv.org/abs/2404.13925,Apr-24,数据集构建,
89,One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation,https://arxiv.org/abs/2402.11683,Feb-24,数据集构建,
90,The GitHub Recent Bugs Dataset for Evaluating LLM-based Debugging Applications,https://arxiv.org/abs/2310.13229,Oct-23,数据集构建,
91,DHP Benchmark: Are LLMs Good NLG Evaluators?,https://arxiv.org/abs/2408.13704,Aug-24,模型评估模型,
92,IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering,https://arxiv.org/abs/2408.13545,Aug-24,模型评估模型,
93,Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates,https://arxiv.org/abs/2408.13006,Aug-24,模型评估模型,
94,Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs,https://arxiv.org/abs/2408.10902,Aug-24,模型评估模型,
95,Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text,https://arxiv.org/abs/2408.09235,Aug-24,模型评估模型,
96,Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge,https://arxiv.org/abs/2408.08808,Aug-24,模型评估模型,
97,Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions,https://arxiv.org/abs/2408.08781,Aug-24,模型评估模型,
98,The Two Sides of the Coin: Hallucination Generation and Detection with LLMs as Evaluators for LLMs,https://arxiv.org/abs/2407.09152,Jul-24,模型评估模型,
99,On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation,https://arxiv.org/abs/2407.03841,Jul-24,模型评估模型,
100,Rethinking LLM-based Preference Evaluation,https://arxiv.org/abs/2407.01085,Jul-24,模型评估模型,
101,LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks,https://arxiv.org/abs/2406.18403,Jun-24,模型评估模型,
102,PARIKSHA : A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data,https://arxiv.org/abs/2406.15053,Jun-24,模型评估模型,
103,Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges,https://arxiv.org/abs/2406.12624,Jun-24,模型评估模型,
104,LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models,https://arxiv.org/abs/2406.09008,Jun-24,模型评估模型,
105,LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation,https://arxiv.org/abs/2406.02863,Jun-24,模型评估模型,
106,Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge,https://arxiv.org/abs/2405.05253,May-24,模型评估模型,
107,Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models,https://arxiv.org/abs/2404.18796,Apr-24,模型评估模型,
108,LLM Evaluators Recognize and Favor Their Own Generations,https://arxiv.org/abs/2404.13076,Apr-24,模型评估模型,
109,On the Limitations of Fine-tuned Judge Models for LLM Evaluation,https://arxiv.org/abs/2403.02839,Mar-24,模型评估模型,
110,Are LLM-based Evaluators Confusing NLG Quality Criteria?,https://arxiv.org/abs/2402.12055,Feb-24,模型评估模型,
111,LLM-based NLG Evaluation: Current Status and Challenges,https://arxiv.org/abs/2402.01383,Feb-24,模型评估模型,
112,Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate,https://arxiv.org/abs/2401.16788,Jan-24,模型评估模型,
113,Fusion-Eval: Integrating Assistant Evaluators with LLMs,https://arxiv.org/abs/2311.09204,Nov-23,模型评估模型,
114,Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs,https://arxiv.org/abs/2311.00681,Nov-23,模型评估模型,
115,Calibrating LLM-Based Evaluator,https://arxiv.org/abs/2309.13308,Oct-23,模型评估模型,
116,Less is More for Long Document Summary Evaluation by LLMs,https://arxiv.org/abs/2309.07382,Sep-23,模型评估模型,
117,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,https://arxiv.org/abs/2308.07201,Aug-23,模型评估模型,
118,LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models,https://arxiv.org/abs/2307.07889,Jul-23,模型评估模型,
119,MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs,https://arxiv.org/abs/2409.02257,Sep-24,数据集构建,推理
120,Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation,https://arxiv.org/abs/2409.00696,Sep-24,通用,
121,LLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization,https://arxiv.org/abs/2409.00630,Sep-24,模型评估模型,摘要
122,Can Large Language Models Reason and Plan?,https://arxiv.org/abs/2403.04121,May-2024,通用,
123,Position: LLMs Can’t Plan But Can Help Planning in LLM-Modulo Frameworks,https://arxiv.org/abs/2402.01817,Feb-2024,通用,推理
124,Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey),https://arxiv.org/abs/2407.12858,Jul-2024,通用,
125,LLMs Still Can’t Plan; Can LRMs? A Preliminary Evaluation of OpenAI’s o1 on PlanBench,https://arxiv.org/abs/2409.13373,Sep-2024,通用,推理
126,Can GPT-O1 Kill All Bugs; An Evaluation of GPT-Family LLMs on QuixBugs,https://arxiv.org/abs/2409.10033,Sep-2024,代码,
127,L3Cube-IndicQuest: A Benchmark Questing Answering Dataset for Evaluating Knowledge of LLMs in Indic Context,https://arxiv.org/abs/2409.08706,Sep-2024,数据集构建,
128,A Dataset for Evaluating LLM-based Evaluation Functions for Research Question Extraction Task,https://arxiv.org/abs/2409.06883,Sep-2024,数据集构建,
129,NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls,https://arxiv.org/abs/2409.03797,Sep-2024,数据集构建,
130,MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs,https://arxiv.org/abs/2409.02257,Sep-2024,数据集构建,